[
  {
    "id": 1,
    "slug": "data-visualization-dashboard",
    "title": "Data Visualization Dashboard",
    "description": "Interactive Plotly Dash experience with sub-200 ms filters over 5M+ rows.",
    "summary": "A Plotly Dash experience that lets analysts explore millions of rows with fluid filtering and storytelling visuals.",
    "technologies": ["Python", "Plotly", "Dash", "FastAPI", "Redis", "AWS"],
    "link": "",
    "image": "/public/images/placeholder.png",
    "impactHeadline": "60% faster exploratory analysis for 60+ analysts and sub-200 ms cross-filters.",
    "highlights": [
      "Reduced exploratory analysis time by 60% through prebuilt insight panels.",
      "Implemented server-side data paging so the dashboard scales to 5M+ records.",
      "Designed adaptive color systems for dark/light palettes."
    ],
    "metrics": [
      "4 analytics squads / 60+ weekly users rely on the dashboard for executive readouts.",
      "p50 filter latency 180 ms, p95 320 ms after caching and query tracing.",
      "35+ branded PDF exports generated per week directly from the UI."
    ],
    "architecture": [
      "Dash UI runs on AWS Fargate behind an Application Load Balancer; heavy queries flow through a FastAPI aggregation layer.",
      "ETL jobs orchestrated with Step Functions hydrate S3 + Redshift while Redis caches recent cross-filter payloads.",
      "CloudWatch dashboards plus structured logging surface latency outliers and error budgets."
    ],
    "ownership": "Owned the ingestion-to-visualization pipeline, Terraform modules, caching strategy, and PDF export service, plus CI/CD (GitHub Actions) and observability dashboards.",
    "content": "<p>The dashboard ingests CSV exports and database snapshots, cleans them with Pandas, and then streams curated data frames into Plotly Dash components. I focused on making interactions feel instantaneousâ€”drill-down filters update cross-highlights in under 200ms thanks to memoized queries and lightweight JSON payloads.</p><p>Once the analysis is complete, analysts can export any state of the dashboard to a branded PDF report. The export pipeline renders the Plotly figures headlessly and stitches them together with narrative text.</p>"
  },
  {
    "id": 2,
    "slug": "task-management-app",
    "title": "Task Management App",
    "description": "Realtime kanban boards, RBAC, and analytics for multi-team planning.",
    "summary": "Realtime collaboration, kanban-style boards, and focus analytics for busy student teams.",
    "technologies": ["Node.js", "Express", "MongoDB", "Socket.IO", "Docker", "AWS"],
    "link": "",
    "image": "/public/images/placeholder.png",
    "impactHeadline": "25+ active teammates plan coursework with <1 s WebSocket fan-out and secure magic links.",
    "highlights": [
      "Delivered live board updates with Socket.IO and optimistic UI patterns.",
      "Role-based access control plus passwordless magic links for onboarding.",
      "Weekly focus insights built with aggregated completion stats."
    ],
    "metrics": [
      "Average WebSocket broadcast latency 640 ms for 150 concurrent clients.",
      "Role-based access rollout cut unauthorized access attempts to zero across 6 months.",
      "Deployed Dockerized stack to Fly.io and ECS with GitHub Actions CI/CD in under 8 minutes."
    ],
    "architecture": [
      "Express REST API + Socket.IO gateway sit behind Nginx; MongoDB Atlas stores boards and leverages change streams for realtime fan-out.",
      "Background workers crunch completion stats nightly and publish insights through the same WebSocket channel.",
      "Infrastructure-as-code scripts provision Fly.io and AWS ECS services plus secrets management."
    ],
    "ownership": "Led end-to-end delivery: schema design, RBAC security review, integration tests, deployment automation, and on-call runbooks for Socket.IO health.",
    "content": "<p>This app grew out of my own need to coordinate coursework, research, and freelance gigs. I built a kanban-first interface with drag-and-drop cards, subtasks, and recurring task templates. Users invite teammates, assign work, and track velocity charts generated nightly.</p><p>The backend exposes a clean REST API plus WebSocket gateways for realtime events. MongoDB change streams keep the Socket.IO layer in sync, and JSON Web Tokens secure every request. I containerized the stack so deployments to Fly.io or Render only take a few minutes.</p>"
  },
  {
    "id": 3,
    "slug": "statistical-analysis-toolkit",
    "title": "Statistical Analysis Toolkit",
    "description": "Pip-installable CLI + notebook templates for hypothesis testing at speed.",
    "summary": "CLI + notebook helpers that wrap SciPy/Pandas patterns into shareable templates.",
    "technologies": ["Python", "Pandas", "SciPy", "statsmodels", "Jupyter", "GitHub Actions"],
    "link": "",
    "image": "/public/images/placeholder.png",
    "impactHeadline": "150+ downloads and three research labs rely on the toolkit for reproducible stats workflows.",
    "highlights": [
      "One-command t-tests, ANOVA, chi-square, and regression diagnostics with Markdown exports.",
      "Auto generates visualizations (violin plots, QQ plots) to accompany reports.",
      "Packaged as a pip-installable module consumed across multiple research teams."
    ],
    "metrics": [
      "CI suite runs 120+ synthetic-dataset tests in under 4 minutes on every push.",
      "PyPI release cadence bi-weekly with semantic versioning and changelog automation.",
      "Documented workflow reduced report prep time by ~3 hours per experiment."
    ],
    "architecture": [
      "Modular CLI orchestrates Pandas + SciPy routines and shells out to nbconvert for notebook rendering.",
      "Reusable plotting templates built with seaborn/matplotlib ensure consistent visuals.",
      "Packaging + publishing automated through GitHub Actions and Trusted Publishers for PyPI."
    ],
    "ownership": "Wrote the module, docs, and teaching labs, onboarded two research assistants, and review incoming issues/PRs just like an open-source maintainer.",
    "content": "<p>The toolkit started as a notebook of helper functions and evolved into a polished package. It standardizes experiment analysis: users point to a CSV, select the hypothesis test, and receive both an interactive notebook and a static Markdown summary with conclusions.</p><p>I leaned heavily on Pandas for data wrangling, SciPy/statsmodels for statistical routines, and nbconvert/WeasyPrint for publishing. Continuous integration runs unit tests against synthetic datasets so every release remains trustworthy.</p>"
  }
]
